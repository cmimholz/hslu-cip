{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "!pip install pandas seaborn matplotlib  openpyxl mariadb openpyxl numpy itables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "\n",
    "import mariadb\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "\n",
    "from itables import init_notebook_mode\n",
    "init_notebook_mode(all_interactive=True)\n",
    "\n",
    "print(\"is loading...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from an Excel file into the pandas DataFrame named 'data'\n",
    "\n",
    "data = pd.read_excel('../Data/GonzalezAlonso_Rodrigo_studentC_stage1.xlsx')\n",
    "\n",
    "# Head to get a preview of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the concise summary of the DataFrame, including the number of non-null values in each column\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type transformation\n",
    "\n",
    "data['TRADING LOCATION'] = data['TRADING LOCATION'].astype(\"category\") # Convert the 'TRADING LOCATION' column data type to 'category'\n",
    "data['ISSUER COUNTRY'] = data['ISSUER COUNTRY'].astype(\"category\") # Convert the 'ISSUER COUNTRY' column data type to 'category\n",
    "data['CCY'] = data['CCY'].astype(\"category\") # Convert the 'CCY' column data type to 'category'\n",
    "\n",
    "# Print the updated summary of the DataFrame\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of unique values and values not present:\n",
    "\n",
    "As the data I am working with belongs to unique and separate business it is important for me to verify that these companies ISINs etc are different and that they all have certain unique identifiers and that there is no missing data (at all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values of the DF\n",
    "\n",
    "for col in data.columns: # Loop through each column in the DataFrame\n",
    "    print(f\" - {col}: {data[col].nunique()} unique values\")  # Print the column name and the number of unique values in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NA values of the DF\n",
    "\n",
    "na_counts = data.isna().sum()  # Count the number of NA values in each column\n",
    "\n",
    "# Loop through the Series and print out the counts in a formatted string\n",
    "for column_name, na_count in na_counts.items():\n",
    "    print(f\" - {column_name}: {na_count} missing values (NA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary study of the data through visualizations, boxplots, histograms and correlation plots.\n",
    "\n",
    "A picture is worth a thousand words so I want to see what my data looks like to see if I have to clean it and there is something abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates boxplots for numerical columns in the df DATA.\n",
    "\n",
    "# Selecting numerical columns from the DataFrame\n",
    "numerical_columns = data.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Creating a figure and axes for the boxplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))# Setup the subplots to have 2 rows and 3 columns\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through the numerical columns and create a boxplot for each\n",
    "for ax, col in zip(axes, numerical_columns):\n",
    "    # Set vert=False for horizontal boxplots\n",
    "    data[col].plot(kind='box', ax=ax)\n",
    "    ax.set_title(f'Boxplot of {col}')\n",
    "    ax.set_xlabel('Value')\n",
    "\n",
    "for i in range(len(numerical_columns), 6):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates boxplots for categorical columns in the df DATA.\n",
    "\n",
    "# List of categorical columns I want to plot\n",
    "specific_columns = ['TRADING LOCATION', 'Subtype', 'CCY']\n",
    "\n",
    "# Create a 2x2 subplot layout\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "\n",
    "# Flatten the array of axes for easy iteration\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through the specific columns and the axes simultaneously\n",
    "for ax, col in zip(axs, specific_columns):\n",
    "    data[col].value_counts().plot(kind='bar', ax=ax, color='skyblue')\n",
    "    ax.set_title(f'Frequency of {col}')\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    ax.yaxis.grid(True, linestyle='--', linewidth=0.7, color='grey')  # Add horizontal grid lines for readability\n",
    "\n",
    "# Adjust the layout so that titles, labels, and ticks don't overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Heatmap of the correlation matrix for the df DATA\n",
    "\n",
    "# Select only the numeric columns for the correlation matrix\n",
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Then compute the correlation matrix\n",
    "corr_matrix = numeric_data.corr()\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export original data to csv\n",
    "\n",
    "#data.to_csv('GonzalezAlonso_Rodrigo_studentC_stage1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification of duplicate rows and their cleaning:\n",
    "\n",
    "Since we will later make merch with another data frame, we want to avoid duplications that could complicate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate rows in the dataframe DATA\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "\n",
    "# Number of duplicate rows\n",
    "num_duplicate_rows = duplicate_rows.shape[0]\n",
    "\n",
    "print(f\" - Number of duplicate rows in the DataFrame: {num_duplicate_rows}\")\n",
    "\n",
    "# Drop duplicates from the original DataFrame\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Re-check for duplicate rows in the updated DataFrame\n",
    "new_duplicate_rows = data[data.duplicated()]\n",
    "\n",
    "# New number of duplicate rows\n",
    "new_num_duplicate_rows = new_duplicate_rows.shape[0]\n",
    "\n",
    "print(f\" - Number of duplicates after dropping them from the DataFrame: {new_num_duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning - getting data dirty\n",
    "\n",
    "Now that we have realized that the data seems quite clean, I  will proceed to clean it definitively. The data itself is already ok since most of it is identifiers extracted from the Six API, which is supposed to be of high quality.\n",
    "\n",
    "That is why I  have decided to get it dirty and clean it to demonstrate my knowledge on the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First cleaning:\n",
    "\n",
    "I have created a function that randomly dirty the data with a 50 percent chance with a very common error, periods and commas are a big problem in the financial world because they can confuse users. I have also created the function that cleans these numbers by assigning them all a point, the standard within the industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Step 1: Randomly Swapping Commas and Dots\n",
    "\n",
    "def dirty_one(value):\n",
    "  \"\"\"\n",
    "  This function randomly swaps commas (,) and dots (.) in a value using regular expressions.\n",
    "\n",
    "  Args:\n",
    "      value: The value to be processed, namely every value of a column.\n",
    "\n",
    "  Returns:\n",
    "      The value with commas and dots potentially swapped or the original value if it's missing (NaN).\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  if pd.notna(value):\n",
    "    # Convert to string for regex handling\n",
    "    value_str = str(value)\n",
    "    # Use regex to find commas (,) or dots (.) and replace them randomly with the other\n",
    "    formatted_value = re.sub(r'[,.]', lambda x: '.' if x.group(0) == ',' and random.random() > 0.5 else ',' if x.group(0) == '.' and random.random() > 0.5 else x.group(0), value_str)\n",
    "    return formatted_value\n",
    "  return value\n",
    "\n",
    "# Apply the function to'ROA', 'ROE', 'ROIC'\n",
    "for col in ['ROA', 'ROE', 'ROIC']:\n",
    "    data[col] = data[col].apply(dirty_one)\n",
    "\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second cleaning:\n",
    "\n",
    "A function that aims to put negative numbers in parentheses has been implemented since many times in the financial world negative numbers can be seen like this. Later we returned it to its usual form since with parentheses the column will become a string, which cannot be operated on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Step 2: Removing Negative Signs and transforming them in parenthesis for ROA ROE ROIC\n",
    "\n",
    "def dirty_two(value):\n",
    "  \"\"\"\n",
    "  This function attempts to remove negative signs from numerical values using regular expressions.\n",
    "\n",
    "  Args:\n",
    "      value: The value to be processed (typically from a DataFrame column).\n",
    "  Returns:\n",
    "      The value with negative signs removed (as a string) or the original value if it's missing (NaN).\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  if pd.notna(value):\n",
    "    # Convert to string for regex handling\n",
    "    value_str = str(value)\n",
    "    # Use regex to find negative numbers and replace with parentheses (incorrect!)\n",
    "    formatted_value = re.sub(r'-([\\d.]+)', r'(\\1)', value_str)\n",
    "    return formatted_value\n",
    "  return value\n",
    "\n",
    "# Columns to change\n",
    "columns_to_modify = ['ROA', 'ROE', 'ROIC']\n",
    "\n",
    "# Apply the function to'ROA', 'ROE', 'ROIC'\n",
    "for col in columns_to_modify:\n",
    "    data[col] = data[col].apply(dirty_two)\n",
    "\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dirty data to csv, stage 2\n",
    "\n",
    "#data.to_csv('GonzalezAlonso_Rodrigo_studentC_stage2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning - fixing the  dirty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Step 1: Swapping Commas and Dots to Dots.\n",
    "\n",
    "def clean_one(value):\n",
    "  \"\"\"\n",
    "  This function cleans a numerical value by replacing commas (,) with dots (.) and converting it to a float (if possible).\n",
    "\n",
    "  Args:\n",
    "      value: The value to be processed, namely every value of a column.\n",
    "\n",
    "  Returns:\n",
    "      The cleaned value as a float (if possible) or the original value if it's missing (NaN).\n",
    "\n",
    "  Handles potential errors during conversion by using 'coerce' in pd.to_numeric. This replaces non-numeric values with NaNs.\n",
    "  \"\"\"\n",
    "\n",
    "  if pd.notna(value):\n",
    "    # Convert to string for consistent handling (in case it's not already)\n",
    "    value_str = str(value)\n",
    "    # Replace commas with dots to ensure consistent decimal representation\n",
    "    cleaned = value_str.replace(',', '.')\n",
    "    # Attempt conversion to float, handling errors with 'coerce'\n",
    "    return pd.to_numeric(cleaned, errors='coerce')\n",
    "  return value\n",
    "\n",
    "# Apply the function to'ROA', 'ROE', 'ROIC'\n",
    "for col in ['ROA', 'ROE', 'ROIC']:\n",
    "    data[col] = data[col].apply(clean_one)\n",
    "\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Step 2: Converting parentheses back to negative signs for ROA ROE ROIC\n",
    "\n",
    "def clean_two(value):\n",
    "    \"\"\"\n",
    "    This function converts values enclosed in parentheses to negative numbers in a DataFrame column.\n",
    "\n",
    "    Args:\n",
    "        value: The value to be processed (typically from a DataFrame column).\n",
    "\n",
    "    Returns:\n",
    "        The value with parentheses converted to negative signs (as a float) \n",
    "        or the original value if it's missing (NaN).\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.notna(value):\n",
    "        # Convert to string for consistent regular expression handling\n",
    "        value_str = str(value)\n",
    "        # Remove commas from the string\n",
    "        value_str = value_str.replace(',', '')\n",
    "        # Use regex to find numbers in parentheses and replace with negative numbers\n",
    "        formatted_value = re.sub(r'\\(([\\d.]+)\\)', r'-\\1', value_str)\n",
    "        # Convert back to float for numerical operations\n",
    "        return float(formatted_value)\n",
    "    return value\n",
    "    \n",
    "# Columns to change\n",
    "columns_to_modify = ['ROA', 'ROE', 'ROIC']\n",
    "\n",
    "# Apply the function to'ROA', 'ROE', 'ROIC'\n",
    "for col in columns_to_modify:\n",
    "    data[col] = data[col].apply(clean_two)\n",
    "\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third cleaning:\n",
    "\n",
    "The component column is capitalized as the only column within the entire dataset, so I have decided to make it more comfortable to read and capitalize the first letter of each of the words through a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply title case to the 'COMPONENT' column\n",
    "data['COMPONENT'] = data['COMPONENT'].apply(lambda x: x.title() if pd.notna(x) else x)\n",
    "\n",
    "# Check the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of new columns and data quality control.\n",
    "\n",
    "Unfortunately I haven't found new columns to do, so I decided to check if the issuer country is OK through the ISIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Country Code from the first 2 characters of ISIN\n",
    "data['Country Code'] = data['ISIN'].str[:2]\n",
    "\n",
    "# Extract Unique Identifier from the remaining characters of ISIN (after position 2)\n",
    "data['Unique Identifier'] = data['ISIN'].str[2:]\n",
    "\n",
    "# Function\n",
    "def compare_columns(data, column1, column2):\n",
    "  \"\"\"\n",
    "  This function compares the number of unique values in two columns of a DataFrame.\n",
    "\n",
    "  Args:\n",
    "      data: The DataFrame containing the columns to compare.\n",
    "      column1: The name of the first column.\n",
    "      column2: The name of the second column.\n",
    "\n",
    "  Prints a message comparing the number of unique values in each column.\n",
    "  \"\"\"\n",
    "\n",
    "  num_distinct_column1 = data[column1].nunique() # Count the number of distinct values in column1\n",
    "  num_distinct_column2 = data[column2].nunique() # Count the number of distinct values in column2\n",
    "  are_equal = num_distinct_column1 == num_distinct_column2 # Check if the number of distinct values are equal\n",
    "\n",
    "  # Print the results in a readable format\n",
    "  print(f\" - Number of distinct values in '{column1}': {num_distinct_column1}\")\n",
    "  print(f\" - Number of distinct values in '{column2}': {num_distinct_column2}\")\n",
    "\n",
    "  if are_equal:\n",
    "      print(f\" - The number of distinct values in '{column1}' and '{column2}' are equal.\")\n",
    "  else:\n",
    "      print(f\" - The number of distinct values in '{column1}' and '{column2}' are different.\")\n",
    "\n",
    "# Compare the number of unique values in 'Country Code' and 'ISSUER COUNTRY' columns\n",
    "compare_columns(data, 'Country Code', 'ISSUER COUNTRY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(data, columns_to_drop):\n",
    "    # Elimina las columnas especificadas\n",
    "    data = data.drop(columns_to_drop, axis=1)\n",
    "    return data\n",
    "\n",
    "columns_to_drop = ['Subtype', 'MIC', \"Unique Identifier\", \"Country Code\"]\n",
    "\n",
    "data = drop_columns(data, columns_to_drop)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None or No to None? Thats the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.replace({np.nan: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export clean data to csv, stage 3\n",
    "\n",
    "#data.to_csv('GonzalezAlonso_Rodrigo_studentC_stage3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MariaDB: Uploading to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mariadb.com/resources/blog/how-to-connect-python-programs-to-mariadb/\n",
    "\n",
    "# Connect to MariaDB Platform\n",
    "try:\n",
    "    conn = mariadb.connect(\n",
    "        user=\"cip_user\",\n",
    "        password=\"cip_pw\",\n",
    "        host=\"127.0.0.1\",                    # 10.177.124.185,  localhost , 127.0.0.1     \n",
    "        #host=\"10.177.124.35\",                    # 10.177.124.35,  localhost , 127.0.0.1    FS23\n",
    "        port=3306, \n",
    "        database=\"CIP\"\n",
    "\n",
    "    )\n",
    "except mariadb.Error as e:\n",
    "    print(f\"Error connecting to MariaDB Platform: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Get Cursor\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over each row in the DataFrame\n",
    "\n",
    "data = data.where(pd.notnull(data), None)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    # Extract data from the DataFrame row using the new column names\n",
    "    component = row['COMPONENT']\n",
    "    isin = row['ISIN']\n",
    "    ticker = row['TICKER']\n",
    "    trading_location = row['TRADING LOCATION']\n",
    "    issuer_country = row['ISSUER COUNTRY']\n",
    "    turnover = row['Turnover']\n",
    "    turnover_in_eur = row['Turnover in EUR']\n",
    "    volume = row['Volume']\n",
    "    roa = row['ROA']\n",
    "    roe = row['ROE']\n",
    "    roic = row['ROIC']\n",
    "    ccy = row['CCY']\n",
    "\n",
    "    # Execute INSERT query\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        INSERT INTO studentC_assign\n",
    "        (`COMPONENT`, `ISIN`, `TICKER`, `TRADING_LOCATION`, `ISSUER_COUNTRY`, \n",
    "        `Turnover`, `Turnover_in_EUR`, `Volume`, `ROA`, `ROE`, `ROIC`, `CCY`) \n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "        cur.execute(query, (component, isin, ticker, trading_location, issuer_country, turnover, \n",
    "                            turnover_in_eur, volume, roa, roe, roic, ccy))\n",
    "    except mariadb.Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Commit changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# After committing, you can also confirm by printing a simple success message\n",
    "print(\"Data insertion complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
