{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into DataFrames\n",
    "# df from Markets Business Insiders\n",
    "aex_df = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/aex_stock_data_stage1.csv')\n",
    "bel_20_df = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/bel_20_stock_data_stage1.csv')\n",
    "cac_40_df = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/cac_40_stock_data_stage1.csv')\n",
    "iseq_20_df = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/iseq_20_stock_data_stage1.csv')\n",
    "obx_df = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/obx_stock_data_stage1.csv')\n",
    "osebx_df = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/osebx_stock_data_stage1.csv')\n",
    "psi_20_df = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/psi_20_stock_data_stage1.csv')\n",
    "mib_df = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/mib_stock_data_stage1.csv')\n",
    "\n",
    "# df from Wikipedia\n",
    "aex_df_wiki = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/aex_wikipedia_data_stage1.csv')\n",
    "bel_20_df_wiki = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/bel_20_wikipedia_data_stage1.csv')\n",
    "cac_40_df_wiki = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/cac_40_wikipedia_data_stage1.csv')\n",
    "iseq_20_df_wiki = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/iseq_20_wikipedia_data_stage1.csv')\n",
    "obx_df_wiki = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/obx_wikipedia_data_stage1.csv')\n",
    "osebx_df_wiki = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/osebx_wikipedia_data_stage1.csv')\n",
    "psi_20_df_wiki = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/psi_20_wikipedia_data_stage1.csv')\n",
    "mib_df_wiki = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/raw_data_stage1/mib_wikipedia_data_stage1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'Index Name' column for each DataFrame\n",
    "aex_df['Index'] = 'AEX'\n",
    "bel_20_df['Index'] = 'BEL_20'\n",
    "cac_40_df['Index'] = 'CAC_40'\n",
    "iseq_20_df['Index Name'] = 'ISEQ_20'\n",
    "obx_df['Index'] = 'OBX'\n",
    "osebx_df['Index'] = 'OSEBX'\n",
    "psi_20_df['Index'] = 'PSI_20'\n",
    "mib_df['Index'] = 'MIB'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEX</td>\n",
       "      <td>ABN Amro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEX</td>\n",
       "      <td>Adyen B.V. Parts Sociales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AEX</td>\n",
       "      <td>Ahold Delhaize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEX</td>\n",
       "      <td>Akzo Nobel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEX</td>\n",
       "      <td>ArcelorMittal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>MIB</td>\n",
       "      <td>TERNA - Trasmissione Elettricita Rete Nazional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>MIB</td>\n",
       "      <td>TIM (ex Telecom Italia)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>MIB</td>\n",
       "      <td>UniCredit S.p.A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>MIB</td>\n",
       "      <td>Unipol Gruppo Finanziario SpA Az.ordinaria pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>MIB</td>\n",
       "      <td>UnipolSai S.p.A. (or UnipolSai Assicurazioni S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Index                                            Company\n",
       "0     AEX                                           ABN Amro\n",
       "1     AEX                          Adyen B.V. Parts Sociales\n",
       "2     AEX                                     Ahold Delhaize\n",
       "3     AEX                                         Akzo Nobel\n",
       "4     AEX                                      ArcelorMittal\n",
       "..    ...                                                ...\n",
       "186   MIB  TERNA - Trasmissione Elettricita Rete Nazional...\n",
       "187   MIB                            TIM (ex Telecom Italia)\n",
       "188   MIB                                   UniCredit S.p.A.\n",
       "189   MIB  Unipol Gruppo Finanziario SpA Az.ordinaria pos...\n",
       "190   MIB  UnipolSai S.p.A. (or UnipolSai Assicurazioni S...\n",
       "\n",
       "[191 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the DataFrames\n",
    "df = pd.concat([aex_df, bel_20_df, cac_40_df,iseq_20_df, obx_df, osebx_df, psi_20_df,mib_df])\n",
    "\n",
    "# Reset the index to create a new unique primary key for each row\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Select only the 'Index Name' and 'Name' columns\n",
    "df = df[['Index', 'Name']]\n",
    "df.rename(columns={'Name': 'Company'}, inplace=True)\n",
    "# Display the first 40 rows of the resulting DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file in the 'data' folder\n",
    "csv_filename = 'combined_stock_data_stage2.csv'\n",
    "df.to_csv(f'C:/Users/Victor/Documents/GitHub/hslu-cip/data/impure_data_stage2/{csv_filename}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Index           Company Ticker\n",
      "0       AEX             Adyen  ADYEN\n",
      "1       AEX             Aegon    AGN\n",
      "2       AEX    Ahold Delhaize     AD\n",
      "3       AEX         AkzoNobel   AKZA\n",
      "4       AEX     ArcelorMittal     MT\n",
      "..      ...               ...    ...\n",
      "295  PSI_20      Galp Energia   GALP\n",
      "296  PSI_20           Ibersol    IBS\n",
      "297  PSI_20  Jerónimo Martins    JMT\n",
      "298  PSI_20        Mota-Engil    EGL\n",
      "299  PSI_20               NOS    NOS\n",
      "\n",
      "[300 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# DataFrames list with their corresponding index names\n",
    "df_to_clean = {\n",
    "    'AEX': aex_df_wiki,\n",
    "    'BEL_20': bel_20_df_wiki,\n",
    "    'CAC_40': cac_40_df_wiki,\n",
    "    'ISEQ_20': iseq_20_df,\n",
    "    'OBX': obx_df_wiki,\n",
    "    'OSEBX': osebx_df_wiki,\n",
    "    'PSI_20': psi_20_df_wiki,\n",
    "    'MIB': mib_df_wiki\n",
    "}\n",
    "\n",
    "# Prepare the dataframes\n",
    "for index_name, df in df_to_clean.items():\n",
    "    # Standardize column names\n",
    "    if 'Ticker symbol' in df.columns:\n",
    "        df.rename(columns={'Ticker symbol': 'Ticker'}, inplace=True)\n",
    "    if 'Name' in df.columns and index_name == 'OSEBX':\n",
    "        df.rename(columns={'Name': 'Company'}, inplace=True)\n",
    "    \n",
    "    # Check if 'Ticker' column exists before cleaning it\n",
    "    if 'Ticker' in df.columns:\n",
    "        df['Ticker'] = df['Ticker'].apply(lambda ticker: re.sub(r'.*:', '', ticker).strip())\n",
    "    \n",
    "    # Add the 'Index' column\n",
    "    df['Index'] = index_name\n",
    "\n",
    "# Combine all the prepared dataframes into one\n",
    "df_wiki = pd.concat(df_to_clean.values())\n",
    "\n",
    "# Select only the required columns and remove duplicates\n",
    "df_wiki = df_wiki[['Index', 'Company', 'Ticker']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Function to clean company names\n",
    "def clean_company_name(name):\n",
    "    if isinstance(name, str):\n",
    "        name = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', name)\n",
    "        name = name.replace('...', ' ')\n",
    "        name = ' '.join(name.split())\n",
    "    return name\n",
    "\n",
    "# Apply the cleaning function to the 'Company' column\n",
    "df_wiki['Company'] = df_wiki['Company'].apply(clean_company_name)\n",
    "\n",
    "# Export the final dataframe to a CSV file\n",
    "csv_filename = 'combined_wiki_data_stage2.csv'\n",
    "df_wiki.to_csv(f'C:/Users/Victor/Documents/GitHub/hslu-cip/data/impure_data_stage2/{csv_filename}', index=False)\n",
    "\n",
    "# Display the first few rows of the final dataframe to verify\n",
    "print(df_wiki.head(300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m mapped_company \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m company2 \u001b[38;5;129;01min\u001b[39;00m data2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompany\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 17\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43msimilar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m max_similarity:\n\u001b[0;32m     19\u001b[0m         max_similarity \u001b[38;5;241m=\u001b[39m similarity\n",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m, in \u001b[0;36msimilar\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilar\u001b[39m(a, b):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSequenceMatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Victor\\anaconda3\\Lib\\difflib.py:619\u001b[0m, in \u001b[0;36mSequenceMatcher.ratio\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mratio\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a measure of the sequences' similarity (float in [0,1]).\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \n\u001b[0;32m    600\u001b[0m \u001b[38;5;124;03m    Where T is the total number of elements in both sequences, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;124;03m    1.0\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 619\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(triple[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m triple \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matching_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _calculate_ratio(matches, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb))\n",
      "File \u001b[1;32mc:\\Users\\Victor\\anaconda3\\Lib\\difflib.py:442\u001b[0m, in \u001b[0;36mSequenceMatcher.get_matching_blocks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatching_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatching_blocks\n\u001b[1;32m--> 442\u001b[0m la, lb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# This is most naturally expressed as a recursive algorithm, but\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# at least one user bumped into extreme use cases that exceeded\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# the recursion limit on their box.  So, now we maintain a list\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# ('queue`) of blocks we still need to look at, and append partial\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# results to `matching_blocks` in a loop; the matches are sorted\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# at the end.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m queue \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m0\u001b[39m, la, \u001b[38;5;241m0\u001b[39m, lb)]\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "# Function to find similarity between two strings\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# Read data from both datasets\n",
    "data1 = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/impure_data_stage2/combined_wiki_data_stage2.csv')\n",
    "data2 = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/impure_data_stage2/combined_stock_data_stage2.csv')\n",
    "combined_wiki_data = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/impure_data_stage2/combined_wiki_data_stage2.csv')\n",
    "\n",
    "company_mapping = {}\n",
    "\n",
    "# Iterate over the companies in data1\n",
    "for company1 in data1['Company']:\n",
    "    max_similarity = 0\n",
    "    mapped_company = \"\"\n",
    "    for company2 in data2['Company']:\n",
    "        similarity = similar(company1, company2)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            mapped_company = company2\n",
    "    company_mapping[company1] = mapped_company\n",
    "    \n",
    "# Iterate over the companies in data2\n",
    "for company2 in data2['Company']:\n",
    "    max_similarity = 0\n",
    "    mapped_company = \"\"\n",
    "    for company1 in data1['Company']:\n",
    "        similarity = similar(company1, company2)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            mapped_company = company1\n",
    "    company_mapping[company2] = mapped_company\n",
    "    \n",
    "\n",
    "# Apply the mapping \n",
    "data1['Company'] = data1['Company'].map(company_mapping)\n",
    "data2['Company'] = data2['Company'].map(company_mapping)\n",
    "\n",
    "# Save the modified dataset to a new CSV file\n",
    "#data1.to_csv('../data/raw_data/data1_mapped.csv', index=False)\n",
    "#data2.to_csv('../data/raw_data/data2_mapped.csv', index=False)\n",
    "\n",
    "# Merge the wiki data to get the tickers\n",
    "combined_final_data = data2.merge(\n",
    "    combined_wiki_data[['Company', 'Ticker']],\n",
    "    on='Company',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "missing_tickers = combined_final_data[combined_final_data['Ticker'].isna()]\n",
    "missing_tickers.head()\n",
    "\n",
    "# Drop duplicates\n",
    "combined_final_data = combined_final_data.drop_duplicates(subset=['Index', 'Company', 'Ticker'])\n",
    "\n",
    "# Apply transformations\n",
    "combined_final_data['Company'] = combined_final_data['Company'].str.upper() \n",
    "\n",
    "# Remove accents\n",
    "combined_final_data['Company'] = combined_final_data['Company'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# Save the result to a CSV file\n",
    "csv_filename = 'combined_final_data_stage2.csv'\n",
    "combined_final_data.to_csv(f'C:/Users/Victor/Documents/GitHub/hslu-cip/data/impure_data_stage2/{csv_filename}', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Function to find similarity between two strings\n",
    "def similar(a, b):\n",
    "    if pd.isna(a) or pd.isna(b):\n",
    "        return 0\n",
    "    else:\n",
    "        return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# Read data from both datasets\n",
    "data1 = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/impure_data_stage2/combined_wiki_data_stage2.csv')\n",
    "data2 = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/impure_data_stage2/combined_stock_data_stage2.csv')\n",
    "combined_wiki_data = pd.read_csv('C:/Users/Victor/Documents/GitHub/hslu-cip/data/impure_data_stage2/combined_wiki_data_stage2.csv')\n",
    "\n",
    "company_mapping = {}\n",
    "\n",
    "# Iterate over the companies in data1\n",
    "for company1 in data1['Company']:\n",
    "    max_similarity = 0\n",
    "    mapped_company = \"\"\n",
    "    for company2 in data2['Company']:\n",
    "        similarity = similar(company1, company2)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            mapped_company = company2\n",
    "    company_mapping[company1] = mapped_company\n",
    "    \n",
    "# Iterate over the companies in data2\n",
    "for company2 in data2['Company']:\n",
    "    max_similarity = 0\n",
    "    mapped_company = \"\"\n",
    "    for company1 in data1['Company']:\n",
    "        similarity = similar(company1, company2)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            mapped_company = company1\n",
    "    company_mapping[company2] = mapped_company\n",
    "    \n",
    "\n",
    "# Apply the mapping \n",
    "data1['Company'] = data1['Company'].map(company_mapping)\n",
    "data2['Company'] = data2['Company'].map(company_mapping)\n",
    "\n",
    "# Save the modified dataset to a new CSV file\n",
    "#data1.to_csv('../data/raw_data/data1_mapped.csv', index=False)\n",
    "#data2.to_csv('../data/raw_data/data2_mapped.csv', index=False)\n",
    "\n",
    "# Merge the wiki data to get the tickers\n",
    "combined_final_data = data2.merge(\n",
    "    combined_wiki_data[['Company', 'Ticker']],\n",
    "    on='Company',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "missing_tickers = combined_final_data[combined_final_data['Ticker'].isna()]\n",
    "missing_tickers.head()\n",
    "\n",
    "# Remove NaN values from all columns\n",
    "combined_final_data = combined_final_data.dropna()\n",
    "\n",
    "# Drop duplicates\n",
    "combined_final_data = combined_final_data.drop_duplicates(subset=['Index', 'Company', 'Ticker'])\n",
    "\n",
    "# Apply transformations\n",
    "combined_final_data['Company'] = combined_final_data['Company'].str.upper() \n",
    "\n",
    "# Remove accents\n",
    "combined_final_data['Company'] = combined_final_data['Company'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# Save the result to a CSV file\n",
    "csv_filename = 'combined_final_data_stage2.csv'\n",
    "combined_final_data.to_csv(f'C:/Users/Victor/Documents/GitHub/hslu-cip/data/impure_data_stage2/{csv_filename}', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
